{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_neural_network.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNIiOLrDq0q+TTeK9p47vli",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonkravchenko2001/NLP-Telegram-Bot/blob/master/Transformer_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYsHbLBdxCH3"
      },
      "source": [
        "## **Transformer Model creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3XBOIeWw6Fg"
      },
      "source": [
        "from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization, SpatialDropout1D, Input, Embedding\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random as python_random\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27RidkbtgKo"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdeuOluNJlBP"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "def pos_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwVZdPIotxpH"
      },
      "source": [
        "### Multihead Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g34hmDcSLfAs"
      },
      "source": [
        "class MultiheadSelfAttention(tf.keras.Model):\n",
        "    def __init__(self, d_model, num_heads, mask = False):\n",
        "        super(MultiheadSelfAttention, self).__init__()\n",
        "        self.depth = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.query_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        self.mask = mask\n",
        "\n",
        "    def create_mask(self, length, width):\n",
        "        mask = np.zeros((length, width), dtype='float32')\n",
        "        for i in range(length):\n",
        "            for j in range(width):\n",
        "                if j > i:\n",
        "                    mask[i,j] = -1e9\n",
        "        return tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "\n",
        "    def call(self,query, key, value):\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "        batch_size = query.shape[0]\n",
        "        query = tf.reshape(query, [batch_size, -1, self.num_heads, self.depth])\n",
        "        query = tf.transpose(query, [0, 2, 1, 3])\n",
        "        key = tf.reshape(key, [batch_size, -1, self.num_heads, self.depth])\n",
        "        key = tf.transpose(key, [0, 2, 1, 3])\n",
        "        value = tf.reshape(value, [batch_size, -1, self.num_heads, self.depth])\n",
        "        value = tf.transpose(value, [0, 2, 1, 3])\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        score /= tf.math.sqrt(tf.dtypes.cast(self.depth, dtype=tf.float32))\n",
        "        if self.mask:\n",
        "            score += self.create_mask(score.shape[-2], score.shape[-1])\n",
        "        alignment = tf.nn.softmax(score, axis=-1)\n",
        "        context = tf.matmul(alignment, value)\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])\n",
        "        context = tf.reshape(context, [batch_size, -1, self.depth * self.num_heads])\n",
        "        output = self.dense(context)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZt61k0zuAT0"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXIYUHkTzxfV"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  \n",
        "    def __init__(self, vocab_size, d_model, num_heads,  num_layers=1, dropout=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.dropouts1 = [Dropout(dropout) for i in range(num_layers)]\n",
        "        self.dropouts2 = [Dropout(dropout) for i in range(num_layers)]\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size+1, d_model)\n",
        "        self.mha = [MultiheadSelfAttention(d_model, num_heads) for _ in range(num_layers)]\n",
        "        self.layer_normalization_1 = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        self.layer_normalization_2 = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        self.dense_1 = [tf.keras.layers.Dense(d_model * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(d_model) for _ in range(num_layers)]\n",
        "        \n",
        "   \n",
        "    def call(self, encoder_inputs):\n",
        "        encoder_outputs = self.embedding(encoder_inputs) #shape (batch_size, max_sequence_length(input_text), d_model)\n",
        "        encoder_outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) \n",
        "        encoder_outputs += pos_encoding(encoder_outputs.shape[1], encoder_outputs.shape[2])\n",
        "        for i in range(self.num_layers):\n",
        "            attention = self.mha[i](query=encoder_outputs, key=encoder_outputs, value=encoder_outputs) #shape (batch_size, max_sequence_length(input_text), emebedding dimension)\n",
        "            attention = self.dropouts1[i](attention)\n",
        "            attention = self.layer_normalization_1[i](encoder_outputs + attention)\n",
        "            encoder_outputs = self.dense_1[i](attention) #shape (batch_size, max_sequence_length(input_text), d_model*4)\n",
        "            encoder_outputs = self.dense_2[i](encoder_outputs)  #shape (batch_size, max_sequence_length(input_text), d_model)\n",
        "            encoder_outputs = self.dropouts2[i](encoder_outputs)\n",
        "            encoder_outputs = self.layer_normalization_2[i](attention + encoder_outputs)\n",
        "        return encoder_outputs  #shape (batch_size, max_sequence_length(input_text), d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOnp6kVUuFqy"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JexUC91mGUtL"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, num_heads, num_layers=1, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size+1, d_model)\n",
        "        self.masked_mha = [MultiheadSelfAttention(d_model, num_heads, mask=True) for _ in range(num_layers)]\n",
        "        self.layer_normalization_1 = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        self.layer_normalization_2 = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        self.layer_normalization_3 = [tf.keras.layers.BatchNormalization() for _ in range(num_layers)]\n",
        "        self.dropouts1 = [tf.keras.layers.Dropout(dropout) for _ in range(num_layers)]\n",
        "        self.dropouts2 = [tf.keras.layers.Dropout(dropout) for _ in range(num_layers)]\n",
        "        self.dropouts3 = [tf.keras.layers.Dropout(dropout) for _ in range(num_layers)]\n",
        "        self.mha = [MultiheadSelfAttention(d_model, num_heads) for _ in range(num_layers)]\n",
        "        self.dense_1 = [tf.keras.layers.Dense(d_model * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(d_model) for _ in range(num_layers)]\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size+1, activation = 'softmax')\n",
        "\n",
        "    def call(self, encoder_outputs, decoder_inputs):\n",
        "        decoder_outputs = self.embedding(decoder_inputs)  #shape (batch_size, max_sequence_length(target_text)-1, d_model)\n",
        "        decoder_outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        decoder_outputs += pos_encoding(decoder_outputs.shape[1], decoder_outputs.shape[2])#shape (batch_size, max_sequence_length(target_text)-1, d_model)\n",
        "        for i in range(self.num_layers):\n",
        "            masked_attention = self.masked_mha[i](query=decoder_outputs, key=decoder_outputs, value=decoder_outputs)#shape (batch_size, max_sequence_length(target_text)-1, d_model)\n",
        "            masked_attention = self.dropouts1[i](masked_attention)\n",
        "            masked_attention = self.layer_normalization_1[i](masked_attention + decoder_outputs)\n",
        "            attention = self.mha[i](query=masked_attention, key=encoder_outputs, value=encoder_outputs) #shape (batch_size, max_sequence_length(target_text)-1, d_model)\n",
        "            attention = self.dropouts2[i](attention)\n",
        "            attention = self.layer_normalization_2[i](attention+masked_attention)\n",
        "            decoder_outputs = self.dense_1[i](attention) #shape (batch_size, max_sequence_length(target_text)-1, d_model*4)\n",
        "            decoder_outputs = self.dense_2[i](decoder_outputs) #shape (batch_size, max_sequence_length(target_text)-1, d_model)\n",
        "            decoder_outputs = self.dropouts3[i](decoder_outputs)\n",
        "            decoder_outputs = self.layer_normalization_3[i](decoder_outputs + attention) #shape (batch_size, max_sequence_length(target_text)-1, d_model)\n",
        "        decoder_outputs = self.dense(decoder_outputs)\n",
        "        return decoder_outputs #shape (batch_size, max_sequence_length(target_text)-1, len(target_dict) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msvX_zRUuJcB"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Op9rm01Qjh-"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, input_vocab_size, target_vocab_size, d_model, num_heads, num_layers_encoder=1, num_layers_decoder=1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(vocab_size=input_vocab_size, d_model=d_model, num_heads=num_heads, num_layers=num_layers_encoder) \n",
        "        self.decoder = Decoder(vocab_size=target_vocab_size,d_model=d_model, num_heads=num_heads, num_layers=num_layers_decoder) \n",
        "    \n",
        "    def call(self, inputs):\n",
        "        encoder_outputs = self.encoder(inputs[0]) #shape (batch_size, max_sequence_length(input_text), d_model)\n",
        "        output = self.decoder(encoder_outputs, inputs[1]) #shape (batch_size, max_sequence_length(target_text)-1, len(target_dict) + 1)\n",
        "    \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzPyAKvJuS4b"
      },
      "source": [
        "## **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl_v5GJbJR41",
        "outputId": "098c4d94-2def-4881-f8d2-49e317e7f2c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5rL3Wr1xvz-"
      },
      "source": [
        "### Parsing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT3eJejf9Z3X"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_zZGruICGxC"
      },
      "source": [
        "def conv_dict(url):\n",
        "    conv_dict = {}\n",
        "    file = open(url, mode='r',encoding='unicode_escape')\n",
        "    for line in file:\n",
        "        line = line.split(' +++$+++ ')\n",
        "        conv_dict[line[0]] = line[4]\n",
        "    return conv_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHf6JdhLCMk4"
      },
      "source": [
        "def parse(url):\n",
        "    file = open(url,mode='r',encoding='unicode_escape')\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    c = 0\n",
        "    for line in file:\n",
        "        line = line.split(' +++$+++ ')[3]\n",
        "        line = line.replace(\"[\",\"\").replace(\"]\",\"\").replace(\", \",\",\").replace(\"'\",\"\").replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
        "        line = line.split(\",\")\n",
        "        for i in range(len(line)-1):\n",
        "            if len(d[line[i]].split()) < 25 and len(d[line[i+1]].split()) <25:\n",
        "                inputs.append(d[line[i]] )\n",
        "                targets.append(\"start_token \" + d[line[i+1]] + \" end_token\")\n",
        "    return inputs,targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUjmUVaSx4vr"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxjBfAWcCQXS"
      },
      "source": [
        "def tokenize(text):\n",
        "    tokenizer = Tokenizer(num_words=20000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^{|}`~\\t\\n')\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    text = tokenizer.texts_to_sequences(text)\n",
        "    return text, tokenizer.word_index, {v: k for k, v in tokenizer.word_index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXU84NPLCUIa"
      },
      "source": [
        "def max_sequence_length(text):\n",
        "    length_list = []\n",
        "    for sentence in text:\n",
        "        l = 0\n",
        "        for _ in sentence:\n",
        "            l += 1\n",
        "        length_list.append(l)\n",
        "    return max(length_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TksehYp1CWpB"
      },
      "source": [
        "def min_sequence_length(text):\n",
        "    length_list = []\n",
        "    for sentence in text:\n",
        "        l = 0\n",
        "        for _ in sentence:\n",
        "            l += 1\n",
        "        length_list.append(l)\n",
        "    return min(length_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-eMPd7tyFTR"
      },
      "source": [
        "### Creating Data Generator for more efficient training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHaV0WY-Hdr2"
      },
      "source": [
        "def batch_generator(x, y, batch_size=128):\n",
        "    for i in range(0, len(x), batch_size):\n",
        "        encoder_inputs = pad_sequences(x[i:i+batch_size], maxlen=max_sequence_length(input_text), padding='post')\n",
        "        decoder_inputs = []\n",
        "        decoder_outputs = []\n",
        "        for j, txt in enumerate(y[i:i+batch_size]):\n",
        "            decoder_inputs.append(txt[:-1])\n",
        "            decoder_outputs.append(txt[1:])\n",
        "        decoder_inputs = pad_sequences(decoder_inputs,maxlen=max_sequence_length(target_text)-1, padding='post')\n",
        "        decoder_outputs = pad_sequences(decoder_outputs,maxlen=max_sequence_length(target_text)-1,padding='post')\n",
        "        yield ([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g921fnq7He-B"
      },
      "source": [
        "d = conv_dict('/content/drive/My Drive/transformer_model/transformer_data/movie_lines.txt')\n",
        "input_text, target_text = parse('/content/drive/My Drive/transformer_model/transformer_data/movie_conversations.txt')\n",
        "input_text = input_text[:102000]\n",
        "target_text = target_text[:102000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AU9SP_W1Hl2v"
      },
      "source": [
        "input_text, input_dict, input_dict_r = tokenize(input_text)\n",
        "target_text, target_dict, target_dict_r = tokenize(target_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szLZwWODh8RC"
      },
      "source": [
        "with open('/content/drive/My Drive/transformer_model/input_dict.json', 'w') as fp:\n",
        "    json.dump(input_dict, fp, indent=4)\n",
        "with open('/content/drive/My Drive/transformer_model/target_dict.json', 'w') as fp:\n",
        "    json.dump(target_dict, fp, indent=4)\n",
        "with open('/content/drive/My Drive/transformer_model/target_dict_r.json', 'w') as fp:\n",
        "    json.dump(target_dict_r, fp, sort_keys=True,indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwiL_U3VRcm2"
      },
      "source": [
        "for inp, tar in zip(input_text[:], target_text[:]):\n",
        "    if len(inp) > 20  or len(tar) > 20 or len(tar) < 3:\n",
        "        target_text.remove(tar)\n",
        "        input_text.remove(inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFW8Gr2WDr1Y"
      },
      "source": [
        "input_text =input_text[:100000]\n",
        "target_text = target_text[:100000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYBwGqiIRhWs"
      },
      "source": [
        "num_heads = 8\n",
        "d_model = 256\n",
        "input_vocab_size=len(input_dict)\n",
        "target_vocab_size=len(target_dict)\n",
        "batch_size=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePndweXjyY-x"
      },
      "source": [
        "### Instantiating Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG0tRq5cRpaH"
      },
      "source": [
        "transformer = Transformer(input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size, d_model=d_model, num_heads=num_heads, num_layers_encoder=1, num_layers_decoder=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGvPbidyys9C"
      },
      "source": [
        "### Custom training scheduler "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxWK1lGtSeQ_"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, model_size, warmup_steps=40000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    self.model_size = model_size\n",
        "    self.model_size = tf.cast(self.model_size, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    return tf.math.rsqrt(self.model_size) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYCUyfgHS6gb"
      },
      "source": [
        "lr = CustomSchedule(d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P4PVN-Cy6fi"
      },
      "source": [
        "### Custom training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDyhrjhjTNrX"
      },
      "source": [
        "optimizer = Adam(lr)\n",
        "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "sparse_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "train_accuracy = tf.keras.metrics.Mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfFn2TqrTSqF"
      },
      "source": [
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        x[0] = tf.cast(x[0], tf.int64)\n",
        "        x[1] = tf.cast(x[1], tf.int64)\n",
        "        y_prediction = transformer(x)\n",
        "        loss = loss_func(y, y_prediction)\n",
        "        accuracy = sparse_accuracy(y, y_prediction)\n",
        "    variables = transformer.trainable_variables\n",
        "    grads = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(grads, variables))\n",
        "    train_loss.update_state(loss)\n",
        "    train_accuracy.update_state(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOhDrpp5tP7O"
      },
      "source": [
        "for epoch in range(200):\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    train_data = batch_generator(input_text, target_text, batch_size=400)\n",
        "    for  i, train_batch  in enumerate(train_data):\n",
        "        x_train = train_batch[0]\n",
        "        y_train = train_batch[1]\n",
        "        train_step(x_train, y_train)\n",
        "    print ('Epoch {} train_loss {:.4f} train_accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkrvP_sfzBm0"
      },
      "source": [
        "### Save Model's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUpCbPRRagh"
      },
      "source": [
        "transformer.save_weights('/content/drive/My Drive/transformer_model/my_model', save_format = 'tf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDhbwmSiH8rY"
      },
      "source": [
        "transformer = Transformer(input_vocab_size=input_vocab_size, target_vocab_size=target_vocab_size, d_model=d_model, num_heads=num_heads, num_layers_encoder=1, num_layers_decoder=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMFQtTEAIE_6",
        "outputId": "2ff9c4a8-ac51-40d6-ae70-d0dd2ab1c594",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "transformer.load_weights('/content/drive/My Drive/transformer_model/my_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f75f4300a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLZBMC2DfkBJ"
      },
      "source": [
        "def predict(sentence):\n",
        "    enc_input = encode(sentence)\n",
        "    print(sentence)\n",
        "    out_words = []\n",
        "    de_input = tf.constant([[1]], dtype=tf.int64)\n",
        "    while True:\n",
        "        de_output = transformer([enc_input, de_input])\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "        out_words.append(target_dict_r[new_word.numpy()[0][0]])\n",
        "        if out_words[-1] == 'end_token' or len(out_words) >= 20:\n",
        "            break\n",
        "    print(de_input)\n",
        "    print(' '.join(out_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcNFESPxtYzb"
      },
      "source": [
        "def encode(sentence):\n",
        "    sentence = sentence.split()\n",
        "    for i, el  in enumerate(sentence):\n",
        "        sentence[i] = input_dict[el]\n",
        "    sentence = pad_sequences([sentence], maxlen=max_sequence_length(input_text), padding='post')\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}